{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ4d0EyRJsyoIhJqO4Ngab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanPenhaRamos/GlosarioSpark/blob/main/GlosarioSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hfi3v65inHJ"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear sesión de Spark"
      ],
      "metadata": {
        "id": "Fw8WN9yvg1_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('prueba_spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()\n",
        "SparkContext = spark.sparkContext\n",
        "sc = SparkContext"
      ],
      "metadata": {
        "id": "hfpwxG3rmh_G"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para acceder a Scala desde Spark2  y pyspark2 desde la shell"
      ],
      "metadata": {
        "id": "9GhRdn1YNRy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sh\n",
        "spark2-shell\n",
        "pyspark2"
      ],
      "metadata": {
        "id": "rFlYCBqAmxpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comandos básicos HDFS"
      ],
      "metadata": {
        "id": "C7w-xjdbULmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sh\n",
        "hdfs dfs -mkdir\n",
        "hdfs dfs -ls\n",
        "hdfs dfs -rm\n",
        "hdfs dfs -put/-get <Destino> <Origen>\n",
        "hdfs dfs -cat"
      ],
      "metadata": {
        "id": "_ERAQFOpUPDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RDDs"
      ],
      "metadata": {
        "id": "g2GjxRgbaxCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear RDDs desde archivos de texto mapeando cada línea como elemento de un array de tipo string"
      ],
      "metadata": {
        "id": "wqxBURm_a7Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDDdesdeTXT = spark.sparkContext.textFile(\"archivo.txt\")\n",
        "\n",
        "RDDdesdeData = sc.textFile(\"pathDeMiData/\")\n",
        "\n",
        "RDDdesdeLogs = spark.sparkContext.textFile(\"mydata/*.log\")\n",
        "\n",
        "RDDdesdeMásDeUnTXT = spark.sparkContext.textFile(\"archivo1.txt,archivo2.txt\")"
      ],
      "metadata": {
        "id": "EgUlCoUra0S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*sc.wholeTextFiles* mapea cada archivo de un directorio como elemento de un único RDD (sólo es útil con archivos pequeños porque lo debe soportar la memoria)\n",
        "\n"
      ],
      "metadata": {
        "id": "hbxLutD8bVem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userRDD = sc.wholeTextFiles(\"NombreDirectorio/\")"
      ],
      "metadata": {
        "id": "WMNzM6NHb2tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDDs desde colecciones**\n",
        "\n",
        "Se usan para testing, generar datos de forma programada, integrar con otras librerías o sistemas o aprendizaje.\n",
        "\n"
      ],
      "metadata": {
        "id": "A1JW5gxefdKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myData = [\"Alice\",\"Carlos\",\"Frank\",\"Barbara\", \"Alice\"]\n",
        "\n",
        "myRDD = sc.parallelize(myData)"
      ],
      "metadata": {
        "id": "0IXiQH8Bfjy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Salvando RDDs**\n",
        "\n",
        "Para salvar el RDD como archivo de texto plano\n",
        "\n",
        "Hay que poner el nombre de un directorio, no de un archivo ya que al ser un archivo distribuido tendrá diferentes nombres."
      ],
      "metadata": {
        "id": "XC_pa19agwpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.saveAsTextFile(\"mydata/\")"
      ],
      "metadata": {
        "id": "dIcU-O6Bg0iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acciones comunes en RDDs:\n",
        "\n",
        "- .count Devuelve el número de elementos\n",
        "\n",
        "- .first Devuelve el primer elemento\n",
        "\n",
        "- .take (n) Devuelve un array (Scala) o una lista (Python) de los primeros n elementos\n",
        "\n",
        "- .collect Devuelve un array (Scala) o una lista (Python) de todos los elementos"
      ],
      "metadata": {
        "id": "Lij3KWArhPAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.count()"
      ],
      "metadata": {
        "id": "bLCZeslthZg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.first()"
      ],
      "metadata": {
        "id": "xghl8kZ0hnuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.take(2)"
      ],
      "metadata": {
        "id": "XM_LxkBzhnjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.collect()"
      ],
      "metadata": {
        "id": "rySsXfSWhnSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplos de transformaciones:\n",
        "\n",
        "- .distinct Crea un nuevo RDD eliminando los elementos duplicados\n",
        "\n",
        "- .union(rdd) Crea un nuevo RDD uniendo la data de un RDD en otro\n",
        "\n",
        "- .map(function) Crea un nuevo RDD ejectuando la función en cada elemento del RDD origen\n",
        "\n",
        "- .filter(function) Crea un nuevo RDD incluyendo o excluyendo cada registro del RDD en base a una función booleana\n",
        "\n",
        "- .flatMap\n",
        "\n",
        "- .mapPartitions"
      ],
      "metadata": {
        "id": "8EiXIk1_h68C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.distinct().collect()"
      ],
      "metadata": {
        "id": "NbnwMKsgh_XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.union(RDDdesdeData)"
      ],
      "metadata": {
        "id": "HGkL1Im1iGjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.map(lambda x: x.upper())"
      ],
      "metadata": {
        "id": "l0ajGKMAiL5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myRDD.filter(lambda x: x.startswith(\"A\"))"
      ],
      "metadata": {
        "id": "pP3XbA1FiOjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformaciones entre RDDs**\n",
        "\n",
        "- .union(RDD) Une **todos** los elementos de los dos RDDs\n",
        "\n",
        "- .intersection(RDD) Devuelve los elementos que tienen en común ambos RDDs\n",
        "\n",
        "- .substract(RDD) Elimina los elementos del RDD parametrizado al RDD original"
      ],
      "metadata": {
        "id": "kCIy2F9ObSz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r1 = [1,2,3,3]\n",
        "r2 = [2,4]\n",
        "\n",
        "rdd1 = sc.parallelize(r1)\n",
        "rdd2 = sc.parallelize(r2)"
      ],
      "metadata": {
        "id": "n-Ff0skGbvg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.union(rdd2).collect()"
      ],
      "metadata": {
        "id": "U7r1NwCUdxov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.intersection(rdd2).collect()"
      ],
      "metadata": {
        "id": "Y8u5Aqqydw9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.subtract(rdd2).collect()"
      ],
      "metadata": {
        "id": "3HY6sckCdwzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames\n",
        "---"
      ],
      "metadata": {
        "id": "b4-ARFICjrfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La SparkSession (necesaria para trabajar con DFs) tiene funciones y atributos para acceder a la funcionalidad de Spark\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOfZtZaWmqKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersschema = StructType([\n",
        "    StructField(\"nombre\", StringType(), True),\n",
        "    StructField(\"apellido\", StringType(), True),\n",
        "    StructField(\"nacionalidad\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "YDVZ6bA-284z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users =[\n",
        "    {   \"nombre\": \"Juana\",\n",
        "        \"apellido\": \"de Arco\",\n",
        "        \"nacionalidad\": \"Francesa\"  },\n",
        "    {   \"nombre\": \"Leonardo\",\n",
        "        \"apellido\": \"da Vinci\",\n",
        "        \"nacionalidad\": \"Italiano\"  },\n",
        "    {   \"nombre\": \"Cleopatra\",\n",
        "        \"apellido\": \"VII\",\n",
        "        \"nacionalidad\": \"Egipcia\"   },\n",
        "    {   \"nombre\": \"Albert\",\n",
        "        \"apellido\": \"Einstein\",\n",
        "        \"nacionalidad\": \"Aleman\"    },\n",
        "    {   \"nombre\": \"Marie\",\n",
        "        \"apellido\": \"Curie\",\n",
        "        \"nacionalidad\": \"Polaca\"    },\n",
        "]"
      ],
      "metadata": {
        "id": "Af7B7FrnAJe-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF = spark.createDataFrame(users)"
      ],
      "metadata": {
        "id": "CVGsgB5omSDJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- spark.read Para obtener datos desde distintas fuentes\n"
      ],
      "metadata": {
        "id": "v2TgszolzFu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersJSONDF = spark.read.json(\"users.json\", schema = usersschema)"
      ],
      "metadata": {
        "id": "5DMNbc5ABKHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- spark.sql (query) Para ejecutar una query habiéndo preparado la view previamente mediante [.createOrReplaceTempView](https://colab.research.google.com/drive/1LwU9sqamD9hfymxMyABUm-jdsOaruPwt#scrollTo=sQrZdPRayaZC)"
      ],
      "metadata": {
        "id": "bPbYATYUzP9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.createOrReplaceTempView(\"users\")\n",
        "spark.sql(\"SELECT apellido FROM users\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exKN-9nTygLW",
        "outputId": "71d1166a-3858-40c8-eb5d-034022e711b3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|apellido|\n",
            "+--------+\n",
            "| de Arco|\n",
            "|da Vinci|\n",
            "|     VII|\n",
            "|Einstein|\n",
            "|   Curie|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- spark.catalog Punto de entrada para el [Catalog API](https://spark.apache.org/docs/3.5.2/api/scala/org/apache/spark/sql/catalog/Catalog.html) para gestionar tablas\n",
        "\n",
        "- spark.conf para configurar spark\n",
        "\n",
        "- spark.sparkContext [Punto de entrada a la API de Spark](https://colab.research.google.com/drive/1LwU9sqamD9hfymxMyABUm-jdsOaruPwt#scrollTo=hfpwxG3rmh_G&line=5&uniqifier=1)"
      ],
      "metadata": {
        "id": "hqsUBwADyYNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.createTable(\"tableUsers\",usersDF)"
      ],
      "metadata": {
        "id": "hvHux4betSD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operaciones con DFs\n",
        "\n",
        "Acciones más comunes:\n",
        "\n",
        "- .count Devuelve el número de filas\n",
        "- .first Devuelve la primera fila\n",
        "- .take (n) Devuelve las primera n filas en forma de array\n",
        "- .show (n) Muestra las primeras n filas en forma de tabla (por defecto muestra 20)\n",
        "- .collect Devuelve todas las filas de un DF como un array\n",
        "- .write Escribe la data en un archivo u otra fuente de data\n"
      ],
      "metadata": {
        "id": "GHrPij7Jtm5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.count()"
      ],
      "metadata": {
        "id": "SI4fRobJ_09l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.first()"
      ],
      "metadata": {
        "id": "csS4av3yBTQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.take(2)"
      ],
      "metadata": {
        "id": "PAzuz8zoBTIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.show(3)"
      ],
      "metadata": {
        "id": "0u1UWpG7BTDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.collect()"
      ],
      "metadata": {
        "id": "o1959qi6BS-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usersDF.write.csv(\"users.csv\")"
      ],
      "metadata": {
        "id": "_BgbvxPlBSzF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las *transformaciones* definen un nuevo DF basado en el actual ya que los DFs son inmutables, al igual que las RDD\n",
        "\n",
        "- *.select*: only the specified columns are included\n",
        "\n",
        "- *.where*: only rows where the specified expression is true are included (synonym for filter)\n",
        "\n",
        "- *.orderBy*: rows are sorted by the specified column(s) (synonym for sort)\n",
        "\n",
        "- *.join*: joins two DataFrames on the specified column(s)\n",
        "\n",
        "- *.limit (n)*: creates a new DataFrame with only the first n rows\n",
        "\n",
        "- *.collect*: returns all the rows in the DataFrame as an array\n",
        "\n",
        "- *.write*: save the data to a file or other data source"
      ],
      "metadata": {
        "id": "FXqdTeQLByS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersnamesDF = usersDF.select(\"nombre\",\"apellido\").orderBy(\"apellido\")\n",
        "usersnamesDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQEMILoPCN9d",
        "outputId": "ca97ec9b-5e23-4431-a364-3b22a17b7393"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|   nombre|apellido|\n",
            "+---------+--------+\n",
            "|    Marie|   Curie|\n",
            "|   Albert|Einstein|\n",
            "|Cleopatra|     VII|\n",
            "| Leonardo|da Vinci|\n",
            "|    Juana| de Arco|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infiriendo un *schema* a un topic en formato JSON\n",
        "\n"
      ],
      "metadata": {
        "id": "FgoWjZG6aI0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%spark\n",
        "\n",
        "val kafkaDF = spark.readStream.format(\"kafka\"). \\\n",
        "                              option(\"kafka.bootstrap.servers\", \"localhost:9092\"). \\\n",
        "                              option(\"subscribe\", \"activations\"). \\\n",
        "                              load()"
      ],
      "metadata": {
        "id": "en-4PQSxa7QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El DataStream `kafkaDF` lee del topic `activations` una data en formato JSON\n",
        "\n",
        "Aquí le definimos un `schema` y se lo aplicamos"
      ],
      "metadata": {
        "id": "jELQd84XbHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val activationsSchema = StructType( List( StructField(\"acct_num\", IntegerType),\n",
        "                                         StructField(\"dev_id\", StringType),\n",
        "                                          StructField(\"phone\", StringType),\n",
        "                                          StructField(\"model\", StringType)))\n",
        "\n",
        "val activationsDF = kafkaDF. select(from_json($\"value\".cast(\"string\"), activationsSchema).alias(\"activation\"))"
      ],
      "metadata": {
        "id": "9UpXDQu6aJPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separar un DataFrame leído en CSV en un DataFrame separado en columnas\n",
        "---"
      ],
      "metadata": {
        "id": "9poGRnJk9JZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "statusDF =  linesDF. \\\n",
        "            withColumn(\"model\", split(linesDF.value, \",\")[0]). \\\n",
        "            withColumn(\"dev_id\", split(linesDF.value, \",\")[1]). \\\n",
        "            withColumn(\"dev_temp\",split(linesDF.value, \",\")[2].cast(\"integer\")). \\\n",
        "            withColumn(\"signal\",split(linesDF.value, \",\")[3].cast(\"integer\"))"
      ],
      "metadata": {
        "id": "hBvUdMqk9WGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# .createOrReplaceTempView(\"nombreDeLaView\")\n",
        "\n",
        "Crea una view de un RDD o un DF para poder usar lenguaje de *spark.SQL*  \n",
        "\n",
        "**COMPLETAR**"
      ],
      "metadata": {
        "id": "sQrZdPRayaZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proccesing Time"
      ],
      "metadata": {
        "id": "sv2IVm8ApFTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "\n",
        "import org.apache.spark.sql.streaming.Trigger.ProcessingTime\n",
        "\n",
        "val sortedModelCountQuery = sortedModelCountDF. writeStream. \\\n",
        "                            outputMode(\"complete\").\\\n",
        "                            format(\"console\"). \\\n",
        "                            option(\"truncate\",\"false\"). \\\n",
        "                            trigger(ProcessingTime(\"5 seconds\")).\\\n",
        "                            start"
      ],
      "metadata": {
        "id": "o6WmMK_iYvcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark SQL Data Types\n",
        "---"
      ],
      "metadata": {
        "id": "RXDa6jO3Ohyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JOINS - Combining and Splitting DataFrames\n",
        "---\n",
        "\n",
        "Joins are expensive in the big-data world\n",
        "- Perform joins early in the process\n",
        "- Amortize the cost over many use cases\n"
      ],
      "metadata": {
        "id": "vf46Gc1WfpTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross join**\n",
        "\n",
        "Use the `crossJoin` DataFrame method to join every row in the left (`scientists`) DataFrame with every row in the right (`offices`) DataFrame:\n",
        "\n",
        "**Crea un producto cartesiano**"
      ],
      "metadata": {
        "id": "FpxD7vfPq5T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.crossJoin(offices).show()\n",
        "\n",
        "# Columns with the same name are not renamed. This is called the Cartesian product of the two DataFrames."
      ],
      "metadata": {
        "id": "IJl_2fizq4bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inner join**\n",
        "\n",
        "Use a join expression and the value `inner` to return only those rows for which the join expression is true:"
      ],
      "metadata": {
        "id": "LRs9WNCHrONH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.join(offices, scientists(\"office_id\") === offices(\"office_id\"), \"inner\").show()\n",
        "\n",
        "# Since the join key has the same name on both DataFrames, we can simplify the join as follows:\n",
        "\n",
        "scientists.join(offices, \"office_id\").show()"
      ],
      "metadata": {
        "id": "dy0HYqU8rPzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left semi join**\n",
        "\n",
        "Use the value `left_semi` to return the rows in the left DataFrame that match rows in the right DataFrame:\n"
      ],
      "metadata": {
        "id": "d2mDpVd8r1H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.join(offices, scientists(\"office_id\") === offices(\"office_id\"), \"left_semi\").show()"
      ],
      "metadata": {
        "id": "wdUHhSoKsAgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left anti join**\n",
        "\n",
        "Use the value `left_anti` to return the rows in the left DataFrame that do not match rows in the right DataFrame:\n"
      ],
      "metadata": {
        "id": "zv6Yi7b7sJk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.join(offices, scientists(\"office_id\") === offices(\"office_id\"), \"left_anti\").show()"
      ],
      "metadata": {
        "id": "UT37OIM6sc0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left outer join**\n",
        "\n",
        "Use the value `left` or `left_outer` to return every row in the left DataFrame with or without matching rows in the right DataFrame:"
      ],
      "metadata": {
        "id": "Xo8s4MdBsjg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.join(offices, scientists(\"office_id\") === offices(\"office_id\"), \"left_outer\").show"
      ],
      "metadata": {
        "id": "1pUKWeGXsq6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Right outer join**\n",
        "\n",
        "Use the value `right` or `right_outer` to return every row in the right\n",
        "\n",
        "DataFrame with or without matching rows in the left DataFrame:"
      ],
      "metadata": {
        "id": "_u1XazvFuiyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.join(offices, scientists(\"office_id\") === offices(\"office_id\"), \"right_outer\").show"
      ],
      "metadata": {
        "id": "GhxG14Z_uvth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full outer join**\n",
        "\n",
        "Use the value `full`, `outer`, or `full_outer` to return the union of the left outer and right outer joins (with duplicates removed):\n"
      ],
      "metadata": {
        "id": "JwZNJbtpuwfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientists.join(offices, scientists(\"office_id\") === offices(\"office_id\"), \"full_outer\").show"
      ],
      "metadata": {
        "id": "s2h_bJymu2V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining Streaming DataFrames\n",
        "\n",
        "Types supported:\n",
        "\n",
        "\n",
        "|Left DataFrame | Right DataFrame | Supported Joins |\n",
        "|-              |                 |                 |\n",
        "|streaming      |static           |inner            |\n",
        "|               |                 |left outer       |\n",
        "|static         |streaming        |inner            |\n",
        "|               |                 |right outer      |\n",
        "|streaming      |streaming        |inner            |\n",
        "|               |                 |left outer       |\n",
        "|               |                 |right outer      |\n"
      ],
      "metadata": {
        "id": "IUyrW3NRAGP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "statusWithAccountDF = statusStreamDF. \\\n",
        "      join(accountDevStaticDF,accountDevStaticDF.account_device_id ==statusStreamDF.device_id)"
      ],
      "metadata": {
        "id": "R5Uv1if-FK13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#En este caso las columnas se llaman igual en los dos DFs\n",
        "\n",
        "joinedDF = statusStreamDF.join(activationsStreamDF,\"dev_id\")"
      ],
      "metadata": {
        "id": "2f0bduAEFPlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL supports the following set operations:\n",
        "\n",
        "- Union\n",
        "- Intersection\n",
        "- Subtraction\n",
        "Spark SQL provides a method to split a DataFrame into random subsets\n"
      ],
      "metadata": {
        "id": "biuWX55QfDEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Union**"
      ],
      "metadata": {
        "id": "hj1OOvnyvS41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver_names = drivers.select(\"first_name\")\n",
        "\n",
        "rider_names = riders.select(\"first_name\")\n",
        "\n",
        "\n",
        "names_union = driver_names.union(rider_names).orderBy(\"first_name\")"
      ],
      "metadata": {
        "id": "bXxWsHVSvbAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that union **does not remove duplicates**. Use the distinct method to remove duplicates"
      ],
      "metadata": {
        "id": "Xlqr6ig0vh_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_distinct = names_union.distinct()"
      ],
      "metadata": {
        "id": "CmGWW6Rlvmft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intersect**\n",
        "\n",
        "Use the intersect method to return rows that exist in both DataFrames"
      ],
      "metadata": {
        "id": "0Y_MB-Wsvrid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_intersect = driver_names.intersect(rider_names).orderBy(\"first_name\")\n"
      ],
      "metadata": {
        "id": "nqhWEe2ov1Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Except**\n",
        "Use the except method to return rows in the left\n",
        "\n",
        "DataFrame that do not exist in the right DataFrame"
      ],
      "metadata": {
        "id": "gjk8rSnYv2fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_subtract = driver_names.except(rider_names).orderBy(\"first_name\")"
      ],
      "metadata": {
        "id": "8gce9s0rv_Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL data types are defined in the *pyspark.sql.types* module\n",
        "\n",
        "Spark SQL supports the following basic data types:\n",
        "- NullType\n",
        "- StringType\n",
        "- Byte array data type\n",
        "  - BinaryType\n",
        "- BooleanType\n",
        "- Integer data types\n",
        "  - ByteType\n",
        "  - ShortType\n",
        "  - IntegerType\n",
        "  - LongType\n",
        "- Fixed-point data type\n",
        "  - DecimalType\n",
        "- Floating-point data types\n",
        "  - FloatType\n",
        "  - DoubleType\n",
        "- Date and time data types\n",
        "  - DateType\n",
        "  - TimestampType\n",
        "\n",
        "Spark also supports the following complex (collection) types:\n",
        "- ArrayType\n",
        "- MapType\n",
        "- StructType\n",
        "\n",
        "Spark SQL provides various methods and functions that can be applied to the various data types\n"
      ],
      "metadata": {
        "id": "KEQil7z6OkOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mazo types\n",
        "\n",
        "[Spark Python API - pyspark.sql.functions.array](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.ArrayType.html#pyspark.sql.types.ArrayType)\n",
        "\n",
        "[Spark Python API - pyspark.sql.functions.create_map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.MapType.html#pyspark.sql.types.MapType)\n",
        "\n",
        "[Spark Python API - pyspark.sql.functions.struct](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType)"
      ],
      "metadata": {
        "id": "k_c9MLS21QIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window functions\n",
        "\n",
        "Spark SQL supports the following window functions:\n",
        "- cume_dist\n",
        "- dense_rank\n",
        "- lag\n",
        "- lead\n",
        "- ntile\n",
        "- percent_rank\n",
        "- rank\n",
        "- row_number\n"
      ],
      "metadata": {
        "id": "g9DqgYt98Hcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sliding window aggregation\n",
        "---"
      ],
      "metadata": {
        "id": "AO1nZNUp5e-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window.timeColumn # reference to column containing time event occurred\n",
        "\n",
        "window.windowDuration(\"1 minute\") # window time length and how often windows are created (Specified as strings such as \"1 minute\" or \"30 seconds\")\n",
        "\n",
        "window.slideDuration"
      ],
      "metadata": {
        "id": "S8yXWdJ18JPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EJEMPLO**\n",
        "\n",
        "Count status messages for selected models for every 10 minutes period\n",
        "- Update count every five minutes\n",
        "\n",
        "Use the timestamp column in status Kafka messages to group by window"
      ],
      "metadata": {
        "id": "nEhTwcjQ6hKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kafkaDF = spark.readStream.format(\"kafka\")...\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "modelsTimeDF = kafkaDF.select(\"timestamp\", \\\n",
        "                split(kafkaDF.value, \",\")[0].alias(\"model\"))\n",
        "\n",
        "windowRoninCountsDF = \\\n",
        "      modelsTimeDF.where(modelsTimeDF.model.startswith(\"Ronin S\")). \\\n",
        "      groupBy(window(\"timestamp\",\"10 minutes\",\"5 minutes\"),\"model\").count()  # Línea a destacar\n",
        "\n",
        "windowRoninCountsQuery = windowRoninCountsDF.writeStream. \\\n",
        "      outputMode(\"complete\").format(\"console\").start()"
      ],
      "metadata": {
        "id": "pThtX7t156Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EJEMPLO DE WATERMARK EN DATASTREAM**"
      ],
      "metadata": {
        "id": "IIkqimZX9FBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelsTimeDF = ...\n",
        "\n",
        "watermarkRoninCountsDF = modelsTimeDF. \\\n",
        "                where(modelsTimeDF.model.startswith(\"Ronin S\")). \\\n",
        "                withWatermark(\"timestamp\", \"1 minute\"). \\   ########\n",
        "                groupBy(window(\"timestamp\",\"10 seconds\",\"5 seconds\"),\"model\").count()\n",
        "\n",
        "watermarkRoninCountsQuery =\n",
        "                watermarkRoninCountsDF.writeStream. \\\n",
        "                outputMode(\"complete\").format(\"console\"). \\\n",
        "                option(\"truncate\",\"false\").start()"
      ],
      "metadata": {
        "id": "tt9ZcQdo9Jiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataSet\n",
        "---"
      ],
      "metadata": {
        "id": "Wmrk0w55fVQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de cómo crear un DataSet sencillo con el método sequence `Seq`"
      ],
      "metadata": {
        "id": "K4jdQOUsfkwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "\n",
        "val strings = Seq(\"a string\",\"another string\")\n",
        "\n",
        "val stringDS = spark.createDataset(strings)\n",
        "\n",
        "stringDS.show\n",
        "\n",
        "+--------------+\n",
        "|         value|\n",
        "+--------------+\n",
        "|      a string|\n",
        "|another string|\n",
        "+--------------+"
      ],
      "metadata": {
        "id": "BIe4GXecfXzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scala case classes are a useful way to represent data in a Dataset\n",
        "- They are often used to create simple data-holding objects in Scala\n",
        "- Instances of case classes are called products\n",
        "\n",
        "Encoders define a Dataset's schema using reflection on the object type\n",
        "- Case class arguments are treated as columns"
      ],
      "metadata": {
        "id": "vCwE_f6ugenS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "\n",
        "case class Name(firstName: String, lastName: String)\n",
        "\n",
        "val names = Seq(Name(\"Fred\",\"Flintstone\"),Name(\"Barney\",\"Rubble\"))\n",
        "\n",
        "\n",
        "# required if not running in shell to uses classes as parameter in datasets\n",
        "import spark.implicits._\n",
        "\n",
        "val nameDS = spark.createDataset(names)\n",
        "\n",
        "nameDS.show()\n",
        "\n",
        "+---------+----------+\n",
        "|firstName|  lastName|\n",
        "+---------+----------+\n",
        "|     Fred|Flintstone|\n",
        "|   Barney|    Rubble|\n",
        "+---------+----------+"
      ],
      "metadata": {
        "id": "hCW56heBf3SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crear un DataSet desde un DataFrame**\n",
        "\n",
        "```\n",
        "% JSON\n",
        "{\"firstName\":\"Grace\",\"lastName\":\"Hopper\"}\n",
        "{\"firstName\":\"Alan\",\"lastName\":\"Turing\"}\n",
        "{\"firstName\":\"Ada\",\"lastName\":\"Lovelace\"}\n",
        "{\"firstName\":\"Charles\",\"lastName\":\"Babbage\"}\n",
        "```"
      ],
      "metadata": {
        "id": "Oj0Tu8Ckihau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "\n",
        "val namesDF = spark.read.json(\"names.json\")\n",
        "\n",
        "case class Name(firstName: String, lastName: String)\n",
        "\n",
        "val namesDS = namesDF.as[Name]"
      ],
      "metadata": {
        "id": "doOI-RJqiTra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}